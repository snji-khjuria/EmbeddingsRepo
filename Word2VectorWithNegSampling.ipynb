{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#configuration variables\n",
    "neg_sample_size = 5\n",
    "embedding_size    = 10\n",
    "vocab_size       = 10000\n",
    "vocab_min_frequency = 0\n",
    "batch_size          = 2\n",
    "fileLocation        = \"./corpus.txt\"\n",
    "table_size          = 100\n",
    "window              = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#essential imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getWord2idx(counter):\n",
    "    word2idx = dict()\n",
    "    index=0\n",
    "    for word, _ in counter:\n",
    "        word2idx[word] = index\n",
    "        index+=1\n",
    "    return word2idx\n",
    "\n",
    "def getDataList(words, word2idx, counter):\n",
    "    data      = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in word2idx:\n",
    "            data.append(word2idx[word])\n",
    "        else:\n",
    "            unk_count+=1\n",
    "            data.append(0)\n",
    "    counter[0][1] = unk_count\n",
    "    return data\n",
    "\n",
    "def getCounterDict(word2idx, counter):\n",
    "    counterDict = {}\n",
    "    for w, c in counter:\n",
    "        counterDict[word2idx[w]] = c\n",
    "    return counterDict\n",
    "\n",
    "def buildVocabulary(filename):\n",
    "    with open(filename) as f:\n",
    "        words = [word for line in f.readlines() for word in line.split()]\n",
    "    total_words = len(words)\n",
    "    counter     = [['UNK', 0]]\n",
    "    counter.extend(list(item) for item in Counter(words).most_common() if item[1]>vocab_min_frequency)\n",
    "    vocab_size = len(counter)\n",
    "    word2idx   = getWord2idx(counter)\n",
    "    counterDict = getCounterDict(word2idx, counter)\n",
    "    data       = getDataList(words, word2idx, counter)\n",
    "    idx2word   = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "    return words, total_words, counter, counterDict, vocab_size, word2idx, data, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words, total_words, counter, counterDict, vocab_size, word2idx, data, idx2word = buildVocabulary(fileLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_labels_and_targets(batch_size):\n",
    "    \n",
    "    labels             = np.zeros([batch_size, 1+neg_sample_size], dtype=np.float32)\n",
    "    labels[:, 0]       = 1.0\n",
    "    targets            = np.ndarray([batch_size, 1+neg_sample_size], dtype=np.int32)\n",
    "    return labels, targets\n",
    "\n",
    "labels, targets = get_labels_and_targets(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "def model(Context, Target, embeddingLayer, lrWeights):\n",
    "    context_embedding    = tf.nn.embedding_lookup(embeddingLayer, Context, name=\"context_embeddings\")\n",
    "    target_embedding     = tf.nn.embedding_lookup(lrWeights, Target, name=\"target_lookup\")\n",
    "    softmax_logits       = tf.nn.sigmoid(tf.matmul(context_embedding, target_embedding, transpose_b=True))\n",
    "    return softmax_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create placeholders, model and then loss and train step.\n",
    "Context        = tf.placeholder(tf.int32, [None, 1], name=\"Context\")\n",
    "Target         = tf.placeholder(tf.int32, [None, 1 + neg_sample_size], name=\"Target\")\n",
    "Y              = tf.placeholder(tf.float32, [None, 1 + neg_sample_size], name=\"Y\")\n",
    "init_width     = 0.5/embedding_size \n",
    "contextLayer   = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -init_width, init_width), name='embed_wts')\n",
    "init_weight    = 1.0/math.sqrt(embedding_size)\n",
    "targetLayer    = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev = init_weight), name='target_wts')\n",
    "softmax_logits = model(Context, Target, contextLayer, targetLayer)\n",
    "loss           = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=softmax_logits, labels=Y))\n",
    "train          = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tasks: 1. create table struture\n",
    "#2. build new batch for each structure\n",
    "#. pass those batches to train and print the loss\n",
    "\n",
    "def fill_table(counter, table_size, idx2word, alpha=0.75):\n",
    "    total_count_power = 0.0\n",
    "    for _, count in counter.items():\n",
    "        total_count_power += math.pow(count, alpha)\n",
    "    word_idx  = 1\n",
    "    table     = np.zeros([table_size], dtype=np.int32)\n",
    "    word_prob = math.pow(counter[word_idx], alpha)/total_count_power\n",
    "    limitCrossed = False\n",
    "    for idx in range(table_size):\n",
    "        table[idx] = word_idx\n",
    "        tableProbNum = float(idx)/table_size\n",
    "        if word_prob<tableProbNum:\n",
    "            word_idx+=1\n",
    "            word_prob+=math.pow(counter[word_idx], alpha)/total_count_power\n",
    "        if word_idx > vocab_size or limitCrossed==True:\n",
    "            limitCrossed=True\n",
    "            word_idx = random(random.randrange(1, vocab_size))\n",
    "    return table\n",
    "table = fill_table(counterDict, table_size, idx2word, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  2  2  2  3  3  3  4  4  4  4  5  5  6  6  7  7  8  8  9\n",
      "  9 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22\n",
      " 22 23 23 24 24 25 25 26 26 27 27 28 28 29 30 30 31 31 32 32 33 33 34 34 35\n",
      " 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47]\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_targets_per_batch(target_per_batch, context, targetTable):\n",
    "    target_per_batch[0]  =context\n",
    "    count = 0\n",
    "    tableSize = len(targetTable)\n",
    "    while count<neg_sample_size:\n",
    "        neg_target = targetTable[random.randrange(tableSize)]\n",
    "        if context!=neg_target:\n",
    "            target_per_batch[count+1] = neg_target\n",
    "            count+=1\n",
    "    return target_per_batch\n",
    "\n",
    "#takes batch size, target:[batch, neg_samples_size] and contexts[batch_size]\n",
    "def sample_targets(batch_size, targets, contexts, targetTable):\n",
    "#     print(\"Total contexts are:- \")\n",
    "#     print(len(contexts))\n",
    "    for batch in range(batch_size):\n",
    "        targetPerBatch    = targets[batch, :]\n",
    "        context           = contexts[batch]\n",
    "        targets[batch, :] = sample_targets_per_batch(targetPerBatch, context, targetTable) \n",
    "    return targets\n",
    "#get contexts of size that much and do samples of targets and now create the training data for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#run single training operation\n",
    "def singleTrainOperation(sess, batch_size, xBatch, targets, positiveTargetBatch):\n",
    "    targets = sample_targets(batch_size, targets, positiveTargetBatch, table)\n",
    "#     print(type(xBatch))\n",
    "#     print(len(xBatch))\n",
    "#     print(type(targets))\n",
    "#     print(type(labels))\n",
    "    sess.run(train, feed_dict={Context:xBatch, Target:targets, Y:labels})\n",
    "    \n",
    "    \n",
    "def fullTraining(sess, filename):\n",
    "    with open(filename) as f:\n",
    "        words = [word for line in f.readlines() for word in line.split()]\n",
    "        currentBatchSize = 0\n",
    "        xContextList     = []\n",
    "        xTargetList      = []\n",
    "        trainingOperations = 0\n",
    "        for idx, word in enumerate(words):\n",
    "            wor_idx = word2idx[word]\n",
    "            reduced_window = random.randrange(1, window)\n",
    "            context = word2idx[words[idx]]\n",
    "            for jdx in xrange(idx-reduced_window, idx+reduced_window):\n",
    "                if jdx<0 or jdx==idx or jdx>=len(words):\n",
    "                    continue\n",
    "                target = word2idx[words[jdx]]\n",
    "                xContextList.append([context])\n",
    "                xTargetList.append(target)\n",
    "                currentBatchSize+=1\n",
    "                if currentBatchSize==batch_size:\n",
    "                    singleTrainOperation(sess, batch_size, xContextList, targets, xTargetList)\n",
    "                    currentBatchSize = 0\n",
    "                    trainingOperations+=1\n",
    "                    if trainingOperations%1==0:\n",
    "                        completeTargets = sample_targets(batch_size, targets, xTargetList, table)\n",
    "                        myLoss = sess.run(loss, feed_dict={Context:xContextList, Target:completeTargets, Y:labels})\n",
    "                        print(\"Loss this batch:- \" )\n",
    "                        print(myLoss)\n",
    "                    xContextList = []\n",
    "                    xTargetList  = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss this batch:- \n",
      "3.58272\n",
      "Loss this batch:- \n",
      "3.58254\n",
      "Loss this batch:- \n",
      "3.587\n",
      "Loss this batch:- \n",
      "3.59747\n",
      "Loss this batch:- \n",
      "3.58961\n",
      "Loss this batch:- \n",
      "3.5849\n",
      "Loss this batch:- \n",
      "3.58239\n",
      "Loss this batch:- \n",
      "3.58081\n",
      "Loss this batch:- \n",
      "3.57663\n",
      "Loss this batch:- \n",
      "3.60052\n",
      "Loss this batch:- \n",
      "3.58189\n",
      "Loss this batch:- \n",
      "3.5738\n",
      "Loss this batch:- \n",
      "3.57694\n",
      "Loss this batch:- \n",
      "3.58195\n",
      "Loss this batch:- \n",
      "3.58882\n",
      "Loss this batch:- \n",
      "3.58384\n",
      "Loss this batch:- \n",
      "3.57385\n",
      "Loss this batch:- \n",
      "3.57357\n",
      "Loss this batch:- \n",
      "3.59244\n",
      "Loss this batch:- \n",
      "3.59503\n",
      "Loss this batch:- \n",
      "3.57119\n",
      "Loss this batch:- \n",
      "3.58252\n",
      "Loss this batch:- \n",
      "3.59103\n",
      "Loss this batch:- \n",
      "3.57106\n",
      "Loss this batch:- \n",
      "3.58966\n",
      "Loss this batch:- \n",
      "3.58012\n",
      "Loss this batch:- \n",
      "3.57467\n",
      "Loss this batch:- \n",
      "3.57738\n",
      "Loss this batch:- \n",
      "3.58734\n",
      "Loss this batch:- \n",
      "3.59572\n",
      "Loss this batch:- \n",
      "3.58566\n",
      "Loss this batch:- \n",
      "3.5809\n",
      "Loss this batch:- \n",
      "3.56582\n",
      "Loss this batch:- \n",
      "3.59394\n",
      "Loss this batch:- \n",
      "3.57744\n",
      "Loss this batch:- \n",
      "3.58871\n",
      "Loss this batch:- \n",
      "3.59005\n",
      "Loss this batch:- \n",
      "3.58233\n",
      "Loss this batch:- \n",
      "3.58307\n",
      "Loss this batch:- \n",
      "3.59869\n",
      "Loss this batch:- \n",
      "3.58626\n",
      "Loss this batch:- \n",
      "3.58167\n",
      "Loss this batch:- \n",
      "3.58001\n",
      "Loss this batch:- \n",
      "3.5751\n",
      "Loss this batch:- \n",
      "3.56918\n",
      "Loss this batch:- \n",
      "3.58816\n",
      "Loss this batch:- \n",
      "3.5749\n",
      "Loss this batch:- \n",
      "3.56809\n",
      "Loss this batch:- \n",
      "3.58914\n",
      "Loss this batch:- \n",
      "3.58383\n",
      "Loss this batch:- \n",
      "3.58106\n",
      "Loss this batch:- \n",
      "3.58508\n"
     ]
    }
   ],
   "source": [
    "sess  = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "fullTraining(sess, fileLocation)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
